{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB\n",
    "显卡架构：安培架构（推荐）\n",
    "内存：16GB"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89b89f64d8f8053d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen\n",
    "\n",
    "接着，运行本代码来切割数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7bd9a514ed09ea6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-18T05:02:34.749308Z",
     "start_time": "2024-01-18T05:02:25.564458Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调 \n",
    "接着，我们仅需哟将陪好的参数配置到命令行中，就可以使用命令行进行高效微调，请注意，这里的 pyhton环境需要换做你的python的绝对环境才能进行有效的调整"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1b7a99923349056"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [01:33<00:00, 13.38s/it]\r\n",
      "--> Model\r\n",
      "\r\n",
      "--> model has 6243.584M params\r\n",
      "\r\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\r\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\r\n",
      "Generating train split: 114599 examples [00:00, 172105.35 examples/s]\r\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\r\n",
      "Generating validation split: 1070 examples [00:00, 59778.16 examples/s]\r\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\r\n",
      "Generating test split: 1070 examples [00:00, 59609.84 examples/s]\r\n",
      "Map (num_proc=16): 100%|███████| 114599/114599 [00:14<00:00, 7912.08 examples/s]\r\n",
      "train_dataset: Dataset({\r\n",
      "    features: ['input_ids', 'labels'],\r\n",
      "    num_rows: 114599\r\n",
      "})\r\n",
      "Map (num_proc=16): 100%|████████████| 1070/1070 [00:01<00:00, 834.20 examples/s]\r\n",
      "val_dataset: Dataset({\r\n",
      "    features: ['input_ids', 'output_ids'],\r\n",
      "    num_rows: 1070\r\n",
      "})\r\n",
      "Map (num_proc=16): 100%|████████████| 1070/1070 [00:01<00:00, 846.54 examples/s]\r\n",
      "test_dataset: Dataset({\r\n",
      "    features: ['input_ids', 'output_ids'],\r\n",
      "    num_rows: 1070\r\n",
      "})\r\n",
      "max_steps is given, it will override any value given in num_train_epochs\r\n",
      "***** Running training *****\r\n",
      "  Num examples = 114,599\r\n",
      "  Num Epochs = 1\r\n",
      "  Instantaneous batch size per device = 4\r\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\r\n",
      "  Gradient Accumulation steps = 1\r\n",
      "  Total optimization steps = 3,000\r\n",
      "  Number of trainable parameters = 1,949,696\r\n",
      "{'loss': 4.8941, 'learning_rate': 4.9833333333333336e-05, 'epoch': 0.0}         \r\n",
      "{'loss': 4.7352, 'learning_rate': 4.966666666666667e-05, 'epoch': 0.0}          \r\n",
      "{'loss': 4.6066, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.0}         \r\n",
      "{'loss': 4.1516, 'learning_rate': 4.933333333333334e-05, 'epoch': 0.0}          \r\n",
      "{'loss': 4.0375, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.0}         \r\n",
      "{'loss': 3.9951, 'learning_rate': 4.9e-05, 'epoch': 0.0}                        \r\n",
      "{'loss': 3.9291, 'learning_rate': 4.883333333333334e-05, 'epoch': 0.0}          \r\n",
      "{'loss': 3.9043, 'learning_rate': 4.866666666666667e-05, 'epoch': 0.0}          \r\n",
      "{'loss': 3.7334, 'learning_rate': 4.85e-05, 'epoch': 0.0}                       \r\n",
      "{'loss': 3.7059, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.0}         \r\n",
      "{'loss': 3.6352, 'learning_rate': 4.8166666666666674e-05, 'epoch': 0.0}         \r\n",
      "{'loss': 3.6273, 'learning_rate': 4.8e-05, 'epoch': 0.0}                        \r\n",
      "{'loss': 3.6951, 'learning_rate': 4.7833333333333335e-05, 'epoch': 0.0}         \r\n",
      "{'loss': 3.684, 'learning_rate': 4.766666666666667e-05, 'epoch': 0.0}           \r\n",
      "{'loss': 3.6842, 'learning_rate': 4.75e-05, 'epoch': 0.01}                      \r\n",
      "{'loss': 3.6219, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.01}        \r\n",
      "{'loss': 3.5736, 'learning_rate': 4.716666666666667e-05, 'epoch': 0.01}         \r\n",
      "{'loss': 3.625, 'learning_rate': 4.7e-05, 'epoch': 0.01}                        \r\n",
      "{'loss': 3.5607, 'learning_rate': 4.683333333333334e-05, 'epoch': 0.01}         \r\n",
      "{'loss': 3.6191, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.01}         \r\n",
      "{'loss': 3.5619, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.01}        \r\n",
      "{'loss': 3.5406, 'learning_rate': 4.633333333333333e-05, 'epoch': 0.01}         \r\n",
      "{'loss': 3.5182, 'learning_rate': 4.6166666666666666e-05, 'epoch': 0.01}        \r\n",
      "{'loss': 3.485, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.01}          \r\n",
      "{'loss': 3.5473, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.01}        \r\n",
      "{'loss': 3.5562, 'learning_rate': 4.566666666666667e-05, 'epoch': 0.01}         \r\n",
      "{'loss': 3.5576, 'learning_rate': 4.55e-05, 'epoch': 0.01}                      \r\n",
      "{'loss': 3.6285, 'learning_rate': 4.5333333333333335e-05, 'epoch': 0.01}        \r\n",
      "{'loss': 3.4609, 'learning_rate': 4.516666666666667e-05, 'epoch': 0.01}         \r\n",
      "{'loss': 3.4916, 'learning_rate': 4.5e-05, 'epoch': 0.01}                       \r\n",
      "{'loss': 3.5379, 'learning_rate': 4.483333333333333e-05, 'epoch': 0.01}         \r\n",
      "{'loss': 3.5711, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.01}         \r\n",
      "{'loss': 3.5367, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.01}        \r\n",
      "{'loss': 3.4486, 'learning_rate': 4.433333333333334e-05, 'epoch': 0.01}         \r\n",
      "{'loss': 3.6121, 'learning_rate': 4.4166666666666665e-05, 'epoch': 0.01}        \r\n",
      "{'loss': 3.5686, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}        \r\n",
      "{'loss': 3.5291, 'learning_rate': 4.383333333333334e-05, 'epoch': 0.01}         \r\n",
      "{'loss': 3.634, 'learning_rate': 4.3666666666666666e-05, 'epoch': 0.01}         \r\n",
      "{'loss': 3.4486, 'learning_rate': 4.35e-05, 'epoch': 0.01}                      \r\n",
      "{'loss': 3.4455, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.01}        \r\n",
      "{'loss': 3.3934, 'learning_rate': 4.316666666666667e-05, 'epoch': 0.01}         \r\n",
      "{'loss': 3.476, 'learning_rate': 4.3e-05, 'epoch': 0.01}                        \r\n",
      "{'loss': 3.3373, 'learning_rate': 4.2833333333333335e-05, 'epoch': 0.02}        \r\n",
      "{'loss': 3.59, 'learning_rate': 4.266666666666667e-05, 'epoch': 0.02}           \r\n",
      "{'loss': 3.5871, 'learning_rate': 4.25e-05, 'epoch': 0.02}                      \r\n",
      "{'loss': 3.4469, 'learning_rate': 4.233333333333334e-05, 'epoch': 0.02}         \r\n",
      "{'loss': 3.482, 'learning_rate': 4.216666666666667e-05, 'epoch': 0.02}          \r\n",
      "{'loss': 3.4881, 'learning_rate': 4.2e-05, 'epoch': 0.02}                       \r\n",
      "{'loss': 3.3887, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.02}         \r\n",
      "{'loss': 3.4689, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.02}         \r\n",
      " 17%|██████▋                                 | 500/3000 [06:26<29:52,  1.39it/s]***** Running Evaluation *****\r\n",
      "  Num examples = 50\r\n",
      "  Batch size = 16\r\n",
      "\r\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001B[A\r\n",
      " 50%|██████████████████████▌                      | 2/4 [00:55<00:55, 27.64s/it]\u001B[A\r\n",
      " 75%|█████████████████████████████████▊           | 3/4 [01:50<00:39, 39.10s/it]\u001B[A\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [02:11<00:00, 32.26s/it]\u001B[ABuilding prefix dict from the default dictionary ...\r\n",
      "Dumping model to file cache /tmp/jieba.cache\r\n",
      "Loading model cost 4.013 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "                                                                                \r\n",
      "\u001B[A{'eval_rouge-1': 29.964653999999996, 'eval_rouge-2': 6.249372000000001, 'eval_rouge-l': 24.333008, 'eval_bleu-4': 0.03147685009302634, 'eval_runtime': 201.0315, 'eval_samples_per_second': 0.249, 'eval_steps_per_second': 0.02, 'epoch': 0.02}\r\n",
      " 17%|██████▋                                 | 500/3000 [09:47<29:52,  1.39it/s]\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [02:16<00:00, 32.26s/it]\u001B[A\r\n",
      "                                                                                \u001B[ASaving model checkpoint to ./output/tmp-checkpoint-500\r\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-500/tokenizer_config.json\r\n",
      "Special tokens file saved in ./output/tmp-checkpoint-500/special_tokens_map.json\r\n",
      "{'loss': 3.4709, 'learning_rate': 4.15e-05, 'epoch': 0.02}                      \r\n",
      "{'loss': 3.5018, 'learning_rate': 4.133333333333333e-05, 'epoch': 0.02}         \r\n",
      "{'loss': 3.6154, 'learning_rate': 4.116666666666667e-05, 'epoch': 0.02}         \r\n",
      "{'loss': 3.4498, 'learning_rate': 4.1e-05, 'epoch': 0.02}                       \r\n",
      "{'loss': 3.4688, 'learning_rate': 4.0833333333333334e-05, 'epoch': 0.02}        \r\n",
      "{'loss': 3.4047, 'learning_rate': 4.066666666666667e-05, 'epoch': 0.02}         \r\n",
      "{'loss': 3.5115, 'learning_rate': 4.05e-05, 'epoch': 0.02}                      \r\n",
      "{'loss': 3.4406, 'learning_rate': 4.0333333333333336e-05, 'epoch': 0.02}        \r\n",
      "{'loss': 3.4264, 'learning_rate': 4.016666666666667e-05, 'epoch': 0.02}         \r\n",
      "{'loss': 3.4184, 'learning_rate': 4e-05, 'epoch': 0.02}                         \r\n",
      "{'loss': 3.5682, 'learning_rate': 3.983333333333333e-05, 'epoch': 0.02}         \r\n",
      "{'loss': 3.5109, 'learning_rate': 3.966666666666667e-05, 'epoch': 0.02}         \r\n",
      "{'loss': 3.5756, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.02}        \r\n",
      "{'loss': 3.5471, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.02}         \r\n",
      "{'loss': 3.5893, 'learning_rate': 3.9166666666666665e-05, 'epoch': 0.02}        \r\n",
      "{'loss': 3.5262, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.02}        \r\n",
      "{'loss': 3.4289, 'learning_rate': 3.883333333333333e-05, 'epoch': 0.02}         \r\n",
      "{'loss': 3.3959, 'learning_rate': 3.866666666666667e-05, 'epoch': 0.02}         \r\n",
      "{'loss': 3.4453, 'learning_rate': 3.85e-05, 'epoch': 0.02}                      \r\n",
      "{'loss': 3.4359, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.02}        \r\n",
      "{'loss': 3.3807, 'learning_rate': 3.816666666666667e-05, 'epoch': 0.02}         \r\n",
      "{'loss': 3.4768, 'learning_rate': 3.8e-05, 'epoch': 0.03}                       \r\n",
      "{'loss': 3.4451, 'learning_rate': 3.7833333333333336e-05, 'epoch': 0.03}        \r\n",
      "{'loss': 3.4217, 'learning_rate': 3.766666666666667e-05, 'epoch': 0.03}         \r\n",
      "{'loss': 3.3881, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.03}        \r\n",
      "{'loss': 3.4061, 'learning_rate': 3.733333333333334e-05, 'epoch': 0.03}         \r\n",
      "{'loss': 3.5697, 'learning_rate': 3.7166666666666664e-05, 'epoch': 0.03}        \r\n",
      "{'loss': 3.4379, 'learning_rate': 3.7e-05, 'epoch': 0.03}                       \r\n",
      "{'loss': 3.4701, 'learning_rate': 3.683333333333334e-05, 'epoch': 0.03}         \r\n",
      "{'loss': 3.4939, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.03}        \r\n",
      "{'loss': 3.4963, 'learning_rate': 3.65e-05, 'epoch': 0.03}                      \r\n",
      "{'loss': 3.6105, 'learning_rate': 3.633333333333333e-05, 'epoch': 0.03}         \r\n",
      "{'loss': 3.3854, 'learning_rate': 3.6166666666666674e-05, 'epoch': 0.03}        \r\n",
      "{'loss': 3.5318, 'learning_rate': 3.6e-05, 'epoch': 0.03}                       \r\n",
      "{'loss': 3.4043, 'learning_rate': 3.5833333333333335e-05, 'epoch': 0.03}        \r\n",
      "{'loss': 3.4168, 'learning_rate': 3.566666666666667e-05, 'epoch': 0.03}         \r\n",
      "{'loss': 3.3918, 'learning_rate': 3.55e-05, 'epoch': 0.03}                      \r\n",
      "{'loss': 3.5369, 'learning_rate': 3.5333333333333336e-05, 'epoch': 0.03}        \r\n",
      "{'loss': 3.3969, 'learning_rate': 3.516666666666667e-05, 'epoch': 0.03}         \r\n",
      "{'loss': 3.3955, 'learning_rate': 3.5e-05, 'epoch': 0.03}                       \r\n",
      "{'loss': 3.5484, 'learning_rate': 3.483333333333334e-05, 'epoch': 0.03}         \r\n",
      "{'loss': 3.4408, 'learning_rate': 3.466666666666667e-05, 'epoch': 0.03}         \r\n",
      "{'loss': 3.4238, 'learning_rate': 3.45e-05, 'epoch': 0.03}                      \r\n",
      "{'loss': 3.3469, 'learning_rate': 3.433333333333333e-05, 'epoch': 0.03}         \r\n",
      "{'loss': 3.4381, 'learning_rate': 3.4166666666666666e-05, 'epoch': 0.03}        \r\n",
      "{'loss': 3.4805, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.03}        \r\n",
      "{'loss': 3.4189, 'learning_rate': 3.3833333333333334e-05, 'epoch': 0.03}        \r\n",
      "{'loss': 3.277, 'learning_rate': 3.366666666666667e-05, 'epoch': 0.03}          \r\n",
      "{'loss': 3.3545, 'learning_rate': 3.35e-05, 'epoch': 0.03}                      \r\n",
      "{'loss': 3.4672, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.03}        \r\n",
      " 33%|█████████████                          | 1000/3000 [16:07<25:29,  1.31it/s]***** Running Evaluation *****\r\n",
      "  Num examples = 50\r\n",
      "  Batch size = 16\r\n",
      "\r\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001B[A\r\n",
      " 50%|██████████████████████▌                      | 2/4 [00:55<00:55, 27.59s/it]\u001B[A\r\n",
      " 75%|█████████████████████████████████▊           | 3/4 [01:50<00:39, 39.08s/it]\u001B[A\r\n",
      "                                                                                \u001B[A\r\n",
      "\u001B[A{'eval_rouge-1': 31.053812000000008, 'eval_rouge-2': 7.032488000000001, 'eval_rouge-l': 24.847724, 'eval_bleu-4': 0.037223643263102936, 'eval_runtime': 144.9028, 'eval_samples_per_second': 0.345, 'eval_steps_per_second': 0.028, 'epoch': 0.03}\r\n",
      " 33%|█████████████                          | 1000/3000 [18:32<25:29,  1.31it/s]\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [02:05<00:00, 29.53s/it]\u001B[A\r\n",
      "                                                                                \u001B[ASaving model checkpoint to ./output/tmp-checkpoint-1000\r\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-1000/tokenizer_config.json\r\n",
      "Special tokens file saved in ./output/tmp-checkpoint-1000/special_tokens_map.json\r\n",
      "{'loss': 3.3201, 'learning_rate': 3.316666666666667e-05, 'epoch': 0.04}         \r\n",
      "{'loss': 3.3213, 'learning_rate': 3.3e-05, 'epoch': 0.04}                       \r\n",
      "{'loss': 3.3725, 'learning_rate': 3.283333333333333e-05, 'epoch': 0.04}         \r\n",
      "{'loss': 3.5311, 'learning_rate': 3.266666666666667e-05, 'epoch': 0.04}         \r\n",
      "{'loss': 3.4779, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.04}        \r\n",
      "{'loss': 3.4121, 'learning_rate': 3.233333333333333e-05, 'epoch': 0.04}         \r\n",
      "{'loss': 3.4137, 'learning_rate': 3.2166666666666665e-05, 'epoch': 0.04}        \r\n",
      "{'loss': 3.4004, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.04}        \r\n",
      "{'loss': 3.3539, 'learning_rate': 3.183333333333334e-05, 'epoch': 0.04}         \r\n",
      "{'loss': 3.5014, 'learning_rate': 3.1666666666666666e-05, 'epoch': 0.04}        \r\n",
      "{'loss': 3.4549, 'learning_rate': 3.15e-05, 'epoch': 0.04}                      \r\n",
      "{'loss': 3.4537, 'learning_rate': 3.1333333333333334e-05, 'epoch': 0.04}        \r\n",
      "{'loss': 3.3891, 'learning_rate': 3.116666666666667e-05, 'epoch': 0.04}         \r\n",
      "{'loss': 3.4342, 'learning_rate': 3.1e-05, 'epoch': 0.04}                       \r\n",
      "{'loss': 3.4137, 'learning_rate': 3.0833333333333335e-05, 'epoch': 0.04}        \r\n",
      "{'loss': 3.3225, 'learning_rate': 3.066666666666667e-05, 'epoch': 0.04}         \r\n",
      "{'loss': 3.3871, 'learning_rate': 3.05e-05, 'epoch': 0.04}                      \r\n",
      "{'loss': 3.3371, 'learning_rate': 3.0333333333333337e-05, 'epoch': 0.04}        \r\n",
      "{'loss': 3.291, 'learning_rate': 3.016666666666667e-05, 'epoch': 0.04}          \r\n",
      "{'loss': 3.4818, 'learning_rate': 3e-05, 'epoch': 0.04}                         \r\n",
      "{'loss': 3.4637, 'learning_rate': 2.9833333333333335e-05, 'epoch': 0.04}        \r\n",
      "{'loss': 3.4316, 'learning_rate': 2.9666666666666672e-05, 'epoch': 0.04}        \r\n",
      "{'loss': 3.3988, 'learning_rate': 2.95e-05, 'epoch': 0.04}                      \r\n",
      "{'loss': 3.4145, 'learning_rate': 2.9333333333333336e-05, 'epoch': 0.04}        \r\n",
      "{'loss': 3.4281, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.04}         \r\n",
      "{'loss': 3.4479, 'learning_rate': 2.9e-05, 'epoch': 0.04}                       \r\n",
      "{'loss': 3.4312, 'learning_rate': 2.8833333333333334e-05, 'epoch': 0.04}        \r\n",
      "{'loss': 3.5016, 'learning_rate': 2.8666666666666668e-05, 'epoch': 0.04}        \r\n",
      "{'loss': 3.3062, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.3604, 'learning_rate': 2.8333333333333335e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.4572, 'learning_rate': 2.816666666666667e-05, 'epoch': 0.05}         \r\n",
      "{'loss': 3.325, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.05}         \r\n",
      "{'loss': 3.4439, 'learning_rate': 2.7833333333333333e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.4568, 'learning_rate': 2.7666666666666667e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.5834, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.1742, 'learning_rate': 2.733333333333333e-05, 'epoch': 0.05}         \r\n",
      "{'loss': 3.6328, 'learning_rate': 2.716666666666667e-05, 'epoch': 0.05}         \r\n",
      "{'loss': 3.3689, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.2676, 'learning_rate': 2.6833333333333333e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.3578, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.4148, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.2797, 'learning_rate': 2.633333333333333e-05, 'epoch': 0.05}         \r\n",
      "{'loss': 3.3643, 'learning_rate': 2.6166666666666668e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.2576, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.3404, 'learning_rate': 2.5833333333333336e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.3992, 'learning_rate': 2.5666666666666666e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.4072, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.4389, 'learning_rate': 2.5333333333333337e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.4801, 'learning_rate': 2.5166666666666667e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.3521, 'learning_rate': 2.5e-05, 'epoch': 0.05}                       \r\n",
      " 50%|███████████████████▌                   | 1500/3000 [24:51<18:50,  1.33it/s]***** Running Evaluation *****\r\n",
      "  Num examples = 50\r\n",
      "  Batch size = 16\r\n",
      "\r\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001B[A\r\n",
      " 50%|██████████████████████▌                      | 2/4 [00:55<00:55, 27.60s/it]\u001B[A\r\n",
      " 75%|█████████████████████████████████▊           | 3/4 [01:50<00:39, 39.06s/it]\u001B[A\r\n",
      "                                                                                \u001B[A\r\n",
      "\u001B[A{'eval_rouge-1': 31.832248, 'eval_rouge-2': 6.936996000000001, 'eval_rouge-l': 23.329044, 'eval_bleu-4': 0.03125366755544977, 'eval_runtime': 223.9336, 'eval_samples_per_second': 0.223, 'eval_steps_per_second': 0.018, 'epoch': 0.05}\r\n",
      " 50%|███████████████████▌                   | 1500/3000 [28:35<18:50,  1.33it/s]\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [02:47<00:00, 45.04s/it]\u001B[A\r\n",
      "                                                                                \u001B[ASaving model checkpoint to ./output/tmp-checkpoint-1500\r\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-1500/tokenizer_config.json\r\n",
      "Special tokens file saved in ./output/tmp-checkpoint-1500/special_tokens_map.json\r\n",
      "{'loss': 3.3199, 'learning_rate': 2.4833333333333335e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.3084, 'learning_rate': 2.466666666666667e-05, 'epoch': 0.05}         \r\n",
      "{'loss': 3.3266, 'learning_rate': 2.45e-05, 'epoch': 0.05}                      \r\n",
      "{'loss': 3.3645, 'learning_rate': 2.4333333333333336e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.3625, 'learning_rate': 2.4166666666666667e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.4258, 'learning_rate': 2.4e-05, 'epoch': 0.05}                       \r\n",
      "{'loss': 3.3863, 'learning_rate': 2.3833333333333334e-05, 'epoch': 0.05}        \r\n",
      "{'loss': 3.5119, 'learning_rate': 2.3666666666666668e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.4246, 'learning_rate': 2.35e-05, 'epoch': 0.06}                      \r\n",
      "{'loss': 3.475, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.06}         \r\n",
      "{'loss': 3.3908, 'learning_rate': 2.3166666666666666e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.3602, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.4072, 'learning_rate': 2.2833333333333334e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.2199, 'learning_rate': 2.2666666666666668e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.3953, 'learning_rate': 2.25e-05, 'epoch': 0.06}                      \r\n",
      "{'loss': 3.4084, 'learning_rate': 2.2333333333333335e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.5637, 'learning_rate': 2.216666666666667e-05, 'epoch': 0.06}         \r\n",
      "{'loss': 3.3912, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.3393, 'learning_rate': 2.1833333333333333e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.3725, 'learning_rate': 2.1666666666666667e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.3826, 'learning_rate': 2.15e-05, 'epoch': 0.06}                      \r\n",
      "{'loss': 3.5148, 'learning_rate': 2.1333333333333335e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.6111, 'learning_rate': 2.116666666666667e-05, 'epoch': 0.06}         \r\n",
      "{'loss': 3.3549, 'learning_rate': 2.1e-05, 'epoch': 0.06}                       \r\n",
      "{'loss': 3.324, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.06}         \r\n",
      "{'loss': 3.4629, 'learning_rate': 2.0666666666666666e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.2934, 'learning_rate': 2.05e-05, 'epoch': 0.06}                      \r\n",
      "{'loss': 3.4223, 'learning_rate': 2.0333333333333334e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.298, 'learning_rate': 2.0166666666666668e-05, 'epoch': 0.06}         \r\n",
      "{'loss': 3.5049, 'learning_rate': 2e-05, 'epoch': 0.06}                         \r\n",
      "{'loss': 3.2779, 'learning_rate': 1.9833333333333335e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.2928, 'learning_rate': 1.9666666666666666e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.5105, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.3561, 'learning_rate': 1.9333333333333333e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.4236, 'learning_rate': 1.9166666666666667e-05, 'epoch': 0.06}        \r\n",
      "{'loss': 3.3551, 'learning_rate': 1.9e-05, 'epoch': 0.06}                       \r\n",
      "{'loss': 3.434, 'learning_rate': 1.8833333333333335e-05, 'epoch': 0.07}         \r\n",
      "{'loss': 3.4268, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.07}         \r\n",
      "{'loss': 3.3637, 'learning_rate': 1.85e-05, 'epoch': 0.07}                      \r\n",
      "{'loss': 3.365, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.07}         \r\n",
      "{'loss': 3.4475, 'learning_rate': 1.8166666666666667e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.4197, 'learning_rate': 1.8e-05, 'epoch': 0.07}                       \r\n",
      "{'loss': 3.3965, 'learning_rate': 1.7833333333333334e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.3264, 'learning_rate': 1.7666666666666668e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.4215, 'learning_rate': 1.75e-05, 'epoch': 0.07}                      \r\n",
      "{'loss': 3.3512, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.5508, 'learning_rate': 1.7166666666666666e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.2871, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.248, 'learning_rate': 1.6833333333333334e-05, 'epoch': 0.07}         \r\n",
      "{'loss': 3.3771, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.07}        \r\n",
      " 67%|██████████████████████████             | 2000/3000 [34:50<12:43,  1.31it/s]***** Running Evaluation *****\r\n",
      "  Num examples = 50\r\n",
      "  Batch size = 16\r\n",
      "\r\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001B[A\r\n",
      " 50%|██████████████████████▌                      | 2/4 [00:55<00:55, 27.64s/it]\u001B[A\r\n",
      " 75%|█████████████████████████████████▊           | 3/4 [01:50<00:39, 39.04s/it]\u001B[A\r\n",
      "                                                                                \u001B[A\r\n",
      "\u001B[A{'eval_rouge-1': 30.845010000000002, 'eval_rouge-2': 6.813848, 'eval_rouge-l': 23.588038, 'eval_bleu-4': 0.032287663219781014, 'eval_runtime': 179.8655, 'eval_samples_per_second': 0.278, 'eval_steps_per_second': 0.022, 'epoch': 0.07}\r\n",
      " 67%|██████████████████████████             | 2000/3000 [37:50<12:43,  1.31it/s]\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [02:03<00:00, 28.61s/it]\u001B[A\r\n",
      "                                                                                \u001B[ASaving model checkpoint to ./output/tmp-checkpoint-2000\r\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-2000/tokenizer_config.json\r\n",
      "Special tokens file saved in ./output/tmp-checkpoint-2000/special_tokens_map.json\r\n",
      "{'loss': 3.4869, 'learning_rate': 1.65e-05, 'epoch': 0.07}                      \r\n",
      "{'loss': 3.2867, 'learning_rate': 1.6333333333333335e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.209, 'learning_rate': 1.6166666666666665e-05, 'epoch': 0.07}         \r\n",
      "{'loss': 3.4568, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.3432, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.5133, 'learning_rate': 1.5666666666666667e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.2795, 'learning_rate': 1.55e-05, 'epoch': 0.07}                      \r\n",
      "{'loss': 3.2266, 'learning_rate': 1.5333333333333334e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.3549, 'learning_rate': 1.5166666666666668e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.5459, 'learning_rate': 1.5e-05, 'epoch': 0.07}                       \r\n",
      "{'loss': 3.4014, 'learning_rate': 1.4833333333333336e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.4785, 'learning_rate': 1.4666666666666668e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.3543, 'learning_rate': 1.45e-05, 'epoch': 0.07}                      \r\n",
      "{'loss': 3.2631, 'learning_rate': 1.4333333333333334e-05, 'epoch': 0.07}        \r\n",
      "{'loss': 3.5314, 'learning_rate': 1.4166666666666668e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.3984, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.3424, 'learning_rate': 1.3833333333333334e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.3277, 'learning_rate': 1.3666666666666666e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.368, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.08}         \r\n",
      "{'loss': 3.2969, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.2998, 'learning_rate': 1.3166666666666665e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.3992, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.4549, 'learning_rate': 1.2833333333333333e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.4021, 'learning_rate': 1.2666666666666668e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.2689, 'learning_rate': 1.25e-05, 'epoch': 0.08}                      \r\n",
      "{'loss': 3.2467, 'learning_rate': 1.2333333333333334e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.4258, 'learning_rate': 1.2166666666666668e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.3709, 'learning_rate': 1.2e-05, 'epoch': 0.08}                       \r\n",
      "{'loss': 3.3506, 'learning_rate': 1.1833333333333334e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.3891, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.449, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.08}         \r\n",
      "{'loss': 3.3727, 'learning_rate': 1.1333333333333334e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.2217, 'learning_rate': 1.1166666666666668e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.2611, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.4551, 'learning_rate': 1.0833333333333334e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.2787, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.4584, 'learning_rate': 1.05e-05, 'epoch': 0.08}                      \r\n",
      "{'loss': 3.4762, 'learning_rate': 1.0333333333333333e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.4582, 'learning_rate': 1.0166666666666667e-05, 'epoch': 0.08}        \r\n",
      "{'loss': 3.4779, 'learning_rate': 1e-05, 'epoch': 0.08}                         \r\n",
      "{'loss': 3.425, 'learning_rate': 9.833333333333333e-06, 'epoch': 0.08}          \r\n",
      "{'loss': 3.3541, 'learning_rate': 9.666666666666667e-06, 'epoch': 0.08}         \r\n",
      "{'loss': 3.2879, 'learning_rate': 9.5e-06, 'epoch': 0.08}                       \r\n",
      "{'loss': 3.4328, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.3352, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.3723, 'learning_rate': 9e-06, 'epoch': 0.09}                         \r\n",
      "{'loss': 3.4123, 'learning_rate': 8.833333333333334e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.3947, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.4299, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.3756, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.09}         \r\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [44:03<06:03,  1.37it/s]***** Running Evaluation *****\r\n",
      "  Num examples = 50\r\n",
      "  Batch size = 16\r\n",
      "\r\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001B[A\r\n",
      " 50%|██████████████████████▌                      | 2/4 [00:22<00:22, 11.46s/it]\u001B[A\r\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:40<00:13, 13.83s/it]\u001B[A\r\n",
      "                                                                                \u001B[A\r\n",
      "\u001B[A{'eval_rouge-1': 32.845492, 'eval_rouge-2': 7.21594, 'eval_rouge-l': 25.871006, 'eval_bleu-4': 0.03679944486448713, 'eval_runtime': 77.5889, 'eval_samples_per_second': 0.644, 'eval_steps_per_second': 0.052, 'epoch': 0.09}\r\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [45:21<06:03,  1.37it/s]\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:56<00:00, 14.46s/it]\u001B[A\r\n",
      "                                                                                \u001B[ASaving model checkpoint to ./output/tmp-checkpoint-2500\r\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-2500/tokenizer_config.json\r\n",
      "Special tokens file saved in ./output/tmp-checkpoint-2500/special_tokens_map.json\r\n",
      "{'loss': 3.3969, 'learning_rate': 8.166666666666668e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.3344, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.5016, 'learning_rate': 7.833333333333333e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.2855, 'learning_rate': 7.666666666666667e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.3441, 'learning_rate': 7.5e-06, 'epoch': 0.09}                       \r\n",
      "{'loss': 3.2555, 'learning_rate': 7.333333333333334e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.415, 'learning_rate': 7.166666666666667e-06, 'epoch': 0.09}          \r\n",
      "{'loss': 3.2836, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.332, 'learning_rate': 6.833333333333333e-06, 'epoch': 0.09}          \r\n",
      "{'loss': 3.3383, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.2633, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.09}        \r\n",
      "{'loss': 3.4201, 'learning_rate': 6.333333333333334e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.3121, 'learning_rate': 6.166666666666667e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.4742, 'learning_rate': 6e-06, 'epoch': 0.09}                         \r\n",
      "{'loss': 3.4881, 'learning_rate': 5.833333333333334e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.3156, 'learning_rate': 5.666666666666667e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.4281, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.1887, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.4955, 'learning_rate': 5.166666666666667e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.3359, 'learning_rate': 5e-06, 'epoch': 0.09}                         \r\n",
      "{'loss': 3.2502, 'learning_rate': 4.833333333333333e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.5271, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.09}         \r\n",
      "{'loss': 3.3238, 'learning_rate': 4.5e-06, 'epoch': 0.1}                        \r\n",
      "{'loss': 3.2275, 'learning_rate': 4.333333333333334e-06, 'epoch': 0.1}          \r\n",
      "{'loss': 3.3678, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.1}          \r\n",
      "{'loss': 3.2994, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.1}          \r\n",
      "{'loss': 3.3092, 'learning_rate': 3.833333333333334e-06, 'epoch': 0.1}          \r\n",
      "{'loss': 3.4512, 'learning_rate': 3.666666666666667e-06, 'epoch': 0.1}          \r\n",
      "{'loss': 3.4043, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.1}         \r\n",
      "{'loss': 3.2865, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.1}         \r\n",
      "{'loss': 3.3316, 'learning_rate': 3.166666666666667e-06, 'epoch': 0.1}          \r\n",
      "{'loss': 3.4201, 'learning_rate': 3e-06, 'epoch': 0.1}                          \r\n",
      "{'loss': 3.392, 'learning_rate': 2.8333333333333335e-06, 'epoch': 0.1}          \r\n",
      "{'loss': 3.4688, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.1}          \r\n",
      "{'loss': 3.4787, 'learning_rate': 2.5e-06, 'epoch': 0.1}                        \r\n",
      "{'loss': 3.3572, 'learning_rate': 2.3333333333333336e-06, 'epoch': 0.1}         \r\n",
      "{'loss': 3.4637, 'learning_rate': 2.166666666666667e-06, 'epoch': 0.1}          \r\n",
      "{'loss': 3.357, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1}          \r\n",
      "{'loss': 3.3527, 'learning_rate': 1.8333333333333335e-06, 'epoch': 0.1}         \r\n",
      "{'loss': 3.3727, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.1}         \r\n",
      "{'loss': 3.1697, 'learning_rate': 1.5e-06, 'epoch': 0.1}                        \r\n",
      "{'loss': 3.3961, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.1}         \r\n",
      "{'loss': 3.5209, 'learning_rate': 1.1666666666666668e-06, 'epoch': 0.1}         \r\n",
      "{'loss': 3.4029, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}         \r\n",
      "{'loss': 3.4768, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.1}          \r\n",
      "{'loss': 3.3658, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.1}          \r\n",
      "{'loss': 3.3986, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.1}          \r\n",
      "{'loss': 3.3131, 'learning_rate': 3.3333333333333335e-07, 'epoch': 0.1}         \r\n",
      "{'loss': 3.3178, 'learning_rate': 1.6666666666666668e-07, 'epoch': 0.1}         \r\n",
      "{'loss': 3.4271, 'learning_rate': 0.0, 'epoch': 0.1}                            \r\n",
      "100%|███████████████████████████████████████| 3000/3000 [51:36<00:00,  1.33it/s]***** Running Evaluation *****\r\n",
      "  Num examples = 50\r\n",
      "  Batch size = 16\r\n",
      "\r\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001B[A\r\n",
      " 50%|██████████████████████▌                      | 2/4 [00:21<00:21, 10.73s/it]\u001B[A\r\n",
      " 75%|█████████████████████████████████▊           | 3/4 [01:16<00:29, 29.19s/it]\u001B[A\r\n",
      "                                                                                \u001B[A\r\n",
      "\u001B[A{'eval_rouge-1': 32.680678, 'eval_rouge-2': 7.145488000000001, 'eval_rouge-l': 25.057014, 'eval_bleu-4': 0.03629949947069401, 'eval_runtime': 152.2427, 'eval_samples_per_second': 0.328, 'eval_steps_per_second': 0.026, 'epoch': 0.1}\r\n",
      "100%|███████████████████████████████████████| 3000/3000 [54:09<00:00,  1.33it/s]\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [02:12<00:00, 38.81s/it]\u001B[A\r\n",
      "                                                                                \u001B[ASaving model checkpoint to ./output/tmp-checkpoint-3000\r\n",
      "tokenizer config file saved in ./output/tmp-checkpoint-3000/tokenizer_config.json\r\n",
      "Special tokens file saved in ./output/tmp-checkpoint-3000/special_tokens_map.json\r\n",
      "\r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 3249.7117, 'train_samples_per_second': 3.693, 'train_steps_per_second': 0.923, 'train_loss': 3.4439713541666666, 'epoch': 0.1}\r\n",
      "100%|███████████████████████████████████████| 3000/3000 [54:09<00:00,  1.08s/it]\r\n",
      "***** Running Prediction *****\r\n",
      "  Num examples = 1070\r\n",
      "  Batch size = 16\r\n",
      "100%|███████████████████████████████████████████| 67/67 [42:34<00:00, 38.13s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 /media/zr/Data/Code/ChatGLM3/venv/bin/python3 finetune_hf.py  data/AdvertiseGen_fix  /media/zr/Data/Models/LLM/chatglm3-6b  configs/lora.yaml"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    }
   },
   "id": "17c87410a24d844f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9418f6c5c264601"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1000  checkpoint-2000  checkpoint-3000\r\n",
      "checkpoint-1500  checkpoint-2500  checkpoint-500\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T07:03:19.390123Z",
     "start_time": "2024-01-18T07:03:19.246666Z"
    }
   },
   "id": "3f22b735175e1c0d"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:24<00:00,  3.50s/it]\r\n",
      "这款网纱拼接的连衣裙，采用不规则的木耳边压褶设计，打造出了个性十足的不规则裙摆，轻松穿出时尚性感。腰部的木耳边拉链套头设计，方便穿脱，又可以修饰腰部线条，显得更加精致。下摆采用百褶设计，修饰腿型，显瘦又显高。\r\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1  /media/zr/Data/Code/ChatGLM3/venv/bin/python3 inference_hf.py output/checkpoint-3000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-18T07:08:13.616364Z",
     "start_time": "2024-01-18T07:07:07.346906Z"
    }
   },
   "id": "5060015c24e97ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18cd83087f096094"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
