{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0369cff7-246e-42e6-82a9-e50a203c5e44",
   "metadata": {},
   "source": [
    "本项目是一个由langchain + RAG + finetun chatglm3 model充当agent完成下游任务的demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1be29-a3b9-4e19-8e99-e77b42ece961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的包\n",
    "import requests\n",
    "import os\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun,\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from typing import List, Optional, Mapping, Any, Tuple, Union\n",
    "from functools import partial\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "from langchain.agents import AgentExecutor\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.agents import BaseSingleActionAgent\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ddb236-eef7-4887-bca6-8634e422d507",
   "metadata": {},
   "source": [
    "在本章节，我们将定义一个可由agent调用的tools。\n",
    "Text_classification_Tool期待调用一个下游finetune好的模型，需要结合context（即finetun过程中的Instruction部分）完成模型的预测。\n",
    "整个过程区分了APITool和functional_Tool：\n",
    "    APITool可以用于接入agent可调用api类工具（如搜索）\n",
    "    functional_Tool可以用于完成下游模型的接入（如文本分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a8693-998f-4b07-a07b-60868c956894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APITool(BaseTool):\n",
    "    name: str = \"\"\n",
    "    description: str = \"\"\n",
    "    url: str = \"\"\n",
    "\n",
    "    def _call_api(self, query):\n",
    "        raise NotImplementedError(\"subclass needs to overwrite this method\")\n",
    "\n",
    "    def _run(\n",
    "            self,\n",
    "            query: str,\n",
    "            run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        return self._call_api(query)\n",
    "\n",
    "    async def _arun(\n",
    "            self,\n",
    "            query: str,\n",
    "            run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        raise NotImplementedError(\"APITool does not support async\")\n",
    "\n",
    "class functional_Tool(BaseTool):\n",
    "    name: str = \"\"\n",
    "    description: str = \"\"\n",
    "    url: str = \"\"\n",
    "\n",
    "    def _call_func(self, query):\n",
    "        raise NotImplementedError(\"subclass needs to overwrite this method\")\n",
    "\n",
    "    def _run(\n",
    "            self,\n",
    "            query: str,\n",
    "            run_manager: Optional[CallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        return self._call_func(query)\n",
    "\n",
    "    async def _arun(\n",
    "            self,\n",
    "            query: str,\n",
    "            run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        raise NotImplementedError(\"APITool does not support async\")\n",
    "\n",
    "# search tool #\n",
    "class SearchTool(APITool):\n",
    "    llm: BaseLanguageModel\n",
    "\n",
    "    name = \"搜索问答\"\n",
    "    description = \"根据用户问题搜索最新的结果，并返回Json格式的结果\"\n",
    "\n",
    "    # search params\n",
    "    google_api_key: str\n",
    "    google_cse_id: str\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    top_k = 2\n",
    "\n",
    "    # QA params\n",
    "    qa_template = \"\"\"\n",
    "    请根据下面带```分隔符的文本来回答问题。\n",
    "    ```{text}```\n",
    "    问题：{query}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(qa_template)\n",
    "    llm_chain: LLMChain = None\n",
    "\n",
    "    def _call_api(self, query):\n",
    "        self.get_llm_chain()\n",
    "        context = self.get_search_result(query)\n",
    "        resp = self.llm_chain.predict(text=context, query=query)\n",
    "        return resp\n",
    "\n",
    "    def get_search_result(self, query):\n",
    "        data = {\"key\": self.google_api_key,\n",
    "                \"cx\": self.google_cse_id,\n",
    "                \"q\": query,\n",
    "                \"lr\": \"lang_zh-CN\"}\n",
    "        results = requests.get(self.url, params=data).json()\n",
    "        results = results.get(\"items\", [])[:self.top_k]\n",
    "        snippets = []\n",
    "        if len(results) == 0:\n",
    "            return(\"No Search Result was found\")\n",
    "        for result in results:\n",
    "            print(\"result:\", result)\n",
    "            text = \"\"\n",
    "            if \"title\" in result:\n",
    "                text += result[\"title\"] + \"。\"\n",
    "            if \"snippet\" in result:\n",
    "                text += result[\"snippet\"]\n",
    "            snippets.append(text)\n",
    "        return(\"\\n\\n\".join(snippets))\n",
    "\n",
    "    def get_llm_chain(self):\n",
    "        if not self.llm_chain:\n",
    "            self.llm_chain = LLMChain(llm=self.llm, prompt=self.prompt)\n",
    "\n",
    "class Text_classification_Tool(functional_Tool):\n",
    "    llm: BaseLanguageModel\n",
    "\n",
    "    name = \"文本分类\"\n",
    "    description = \"用户输入句子，完成文本分类\"\n",
    "\n",
    "    # QA params\n",
    "    qa_template = \"\"\"\n",
    "    请根据下面带```分隔符的文本来回答问题。\n",
    "    ```{text}```\n",
    "    问题：{query}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(qa_template)\n",
    "    llm_chain: LLMChain = None\n",
    "\n",
    "    def _call_func(self, query) -> str:\n",
    "        self.get_llm_chain()\n",
    "        context = \"Instruction: 深呼吸，你是一个文本分类模型。你需要根据我的输入给出这句话的情感，候选的情感为：开心、难过、平静\"\n",
    "        resp = self.llm_chain.predict(text=context, query=query)\n",
    "        return resp\n",
    "\n",
    "    def get_llm_chain(self):\n",
    "        if not self.llm_chain:\n",
    "            self.llm_chain = LLMChain(llm=self.llm, prompt=self.prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b22e8-e1ee-4149-b6fa-667f7c811c46",
   "metadata": {},
   "source": [
    "在本章节，我们按照langchain的格式定义一个Chatglm3。\n",
    "其核心函数为_call，内部调用generate_resp生成回复\n",
    "load_model和load_model_from_checkpoint分别用于加载原始chatglm3 model和一个finetune好的model作为下游agent。\n",
    "这里的finetune model使用的是 https://github.com/THUDM/ChatGLM3/tree/main/finetune_basemodel_demo 中的方法训练得到的model\n",
    "训练数据为文本分类数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa67ab-47cf-4791-9372-34b809abc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGLM3(LLM):\n",
    "\n",
    "    model_path: str\n",
    "    max_length: int = 8192\n",
    "    temperature: float = 0.1\n",
    "    top_p: float = 0.7\n",
    "    history: List = []\n",
    "    streaming: bool = True\n",
    "    model: object = None\n",
    "    tokenizer: object = None\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chatglm3-6B\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        add_history: bool = False\n",
    "    ) -> str:\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise RuntimeError(\"Must call `load_model()` to load model and tokenizer!\")\n",
    "\n",
    "        if self.streaming:\n",
    "            text_callback = partial(StreamingStdOutCallbackHandler().on_llm_new_token, verbose=True)\n",
    "            resp = self.generate_resp(prompt, text_callback, add_history=add_history)\n",
    "        else:\n",
    "            resp = self.generate_resp(self, prompt, add_history=add_history)\n",
    "        return resp\n",
    "\n",
    "    def generate_resp(self, prompt, text_callback=None, add_history=True):\n",
    "        resp = \"\"\n",
    "        index = 0\n",
    "        if text_callback:\n",
    "            for i, (resp, _) in enumerate(self.model.stream_chat(\n",
    "                    self.tokenizer,\n",
    "                    prompt,\n",
    "                    self.history,\n",
    "                    max_length=self.max_length,\n",
    "                    top_p=self.top_p,\n",
    "                    temperature=self.temperature\n",
    "            )):\n",
    "                if add_history:\n",
    "                    if i == 0:\n",
    "                        self.history += [[prompt, resp]]\n",
    "                    else:\n",
    "                        self.history[-1] = [prompt, resp]\n",
    "                text_callback(resp[index:])\n",
    "                index = len(resp)\n",
    "        else:\n",
    "            resp, _ = self.model.chat(\n",
    "                self.tokenizer,\n",
    "                prompt,\n",
    "                self.history,\n",
    "                max_length=self.max_length,\n",
    "                top_p=self.top_p,\n",
    "                temperature=self.temperature\n",
    "            )\n",
    "            if add_history:\n",
    "                self.history += [[prompt, resp]]\n",
    "        return resp\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model is not None or self.tokenizer is not None:\n",
    "            return\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(self.model_path, trust_remote_code=True).half().cuda().eval()\n",
    "\n",
    "    def load_model_from_checkpoint(self, checkpoint=None):\n",
    "        if self.model is not None or self.tokenizer is not None:\n",
    "            return\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(self.model_path, trust_remote_code=True).half()\n",
    "        peft_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                inference_mode=False,\n",
    "                r=8,\n",
    "                target_modules=['query_key_value'],\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, peft_config).to(\"cuda\")\n",
    "        if checkpoint==\"text_classification\":\n",
    "            model_dir = \"./emo_classifcation/checkpoint/\" # 换成自己的finetune后的模型\n",
    "            peft_path = \"{}/chatglm-lora.pt\".format(model_dir)\n",
    "            if os.path.exists(peft_path):\n",
    "                 self.model.load_state_dict(torch.load(peft_path), strict=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7069e765-115a-4831-a965-fd5b42bfb50e",
   "metadata": {},
   "source": [
    "在本章节中，我们需要完成一个意图识别agent，用意图识别结果充当路由，链接到下游的的tools。\n",
    "在本项目中，需要agent能通过输入的文本，判断其是一个文本分类任务。从而选择Text_classification_Tool，这需要一个外挂知识。\n",
    "这里采用RAG的方式，通过检索出的知识来辅助LLM做意图识别。\n",
    "包含一个IntentAgent，根据知识库召回的知识填充intent_template\n",
    "利用select_tools = [(name, resp.index(name)) for name in tool_names if name in resp]判断下游应该使用哪个tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c126e-0993-494d-bb71-70665fec9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG模块\n",
    "def process_data(file_path):\n",
    "    all_content = []\n",
    "    files = os.listdir(file_path)\n",
    "    with open(path,encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for content in lines:\n",
    "            all_content.append(content)\n",
    "    return all_content\n",
    "\n",
    "class DFaiss:\n",
    "    def __init__(self):\n",
    "        self.index = faiss.IndexFlatL2(4096)\n",
    "        self.text_str_list = []\n",
    "\n",
    "    def search(self, emb):\n",
    "        distance = 100000\n",
    "        D,I = self.index.search(emb.astype(np.float32), distance)\n",
    "        content = \"\"\n",
    "        for i in range(len(self.text_str_list)):\n",
    "            if D[0][i] < distance:\n",
    "                content += self.text_str_list[I[0][i]]\n",
    "        return content\n",
    "\n",
    "class emb_model:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b-base\", trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(\"THUDM/chatglm3-6b-base\", trust_remote_code=True).half().cuda()\n",
    "        self.myfaiss = DFaiss()\n",
    "\n",
    "    def retrive(self, text):\n",
    "        emb = self.get_sentence_emb(text,is_numpy=True)\n",
    "        retrive_know = self.myfaiss.search(emb)\n",
    "        return retrive_know\n",
    "      \n",
    "    def load_data(self,path):\n",
    "        all_content = process_data(path)\n",
    "        for content in all_content:\n",
    "            self.myfaiss.text_str_list.append(content)\n",
    "            emb = self.get_sentence_emb(content,is_numpy=True)\n",
    "            self.myfaiss.index.add(emb.astype(np.float32))\n",
    "\n",
    "    def get_sentence_emb(self,text,is_numpy=False):\n",
    "        idx = self.tokenizer([text],return_tensors=\"pt\")\n",
    "        idx = idx[\"input_ids\"].to(\"cuda\")\n",
    "        emb = self.model.transformer(idx,return_dict=False)[0]\n",
    "        emb = emb.transpose(0,1)\n",
    "        emb = emb[:,-1]\n",
    "\n",
    "        if is_numpy:\n",
    "            emb = emb.detach().cpu().numpy()\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbae02-b5ac-4c5b-9af3-71fe132255a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent模块\n",
    "class IntentAgent(BaseSingleActionAgent):\n",
    "    tools: List\n",
    "    llm: BaseLanguageModel\n",
    "    intent_template: str = \"\"\"\n",
    "    有一些参考资料，为:{docs}\n",
    "    你的任务是根据「参考资料」来理解用户问题的意图，并判断该问题属于哪一类意图。\n",
    "    用户问题：“{query}”\n",
    "    \"\"\"\n",
    "    #intent_template: str = \"\"\"\n",
    "    #当需要你对一段文本做情感分类的时候，你的回答应该是：文本分类。你的回答应该是：文本分类。你的回答应该是：文本分类。你的回答应该是：文本分类。你的回答应该是：文本分类。不要会带别的，只回答四个字，文本分类\n",
    "    #\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(intent_template)\n",
    "    llm_chain: LLMChain = None\n",
    "\n",
    "    def get_llm_chain(self):\n",
    "        if not self.llm_chain:\n",
    "            self.llm_chain = LLMChain(llm=self.llm, prompt=self.prompt)\n",
    "\n",
    "    def choose_tools(self, query):\n",
    "        self.get_llm_chain()\n",
    "        tool_names = [tool.name for tool in self.tools]\n",
    "        ret_model = emb_model() # RAG\n",
    "        ret_model.load_data(\"./doc/\")\n",
    "        docs = ret_model.retrive(query)\n",
    "        resp = self.llm_chain.predict(query=query, docs=docs)\n",
    "        select_tools = [(name, resp.index(name)) for name in tool_names if name in resp]\n",
    "        select_tools.sort(key=lambda x:x[1])\n",
    "        return [x[0] for x in select_tools]\n",
    "\n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        return [\"input\"]\n",
    "\n",
    "    def plan(\n",
    "            self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        tool_name = self.choose_tools(kwargs[\"input\"])[0]\n",
    "        return AgentAction(tool=tool_name, tool_input=kwargs[\"input\"], log=\"\")\n",
    "\n",
    "    async def aplan(\n",
    "            self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n",
    "    ) -> Union[List[AgentAction], AgentFinish]:\n",
    "        raise NotImplementedError(\"IntentAgent does not support async\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9691ac-8bc1-4aa0-b299-bd2d036574d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "调用主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66394419-af1c-4fec-a30a-1895d591e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google search api ley\n",
    "GOOGLE_API_KEY = \"\"\n",
    "GOOGLE_CSE_ID = \"\"\n",
    "model_path = \"THUDM/chatglm3-6b-base\"\n",
    "llm_base = ChatGLM3(model_path=model_path) \n",
    "llm_text_cls = ChatGLM3(model_path=model_path)\n",
    "# 模型加载\n",
    "llm_base.load_model()\n",
    "llm_text_cls.load_model_from_checkpoint(checkpoint=\"text_classification\")\n",
    "# 下游可路由的工具列表\n",
    "tools = [SearchTool(llm=llm_ori, google_api_key=GOOGLE_API_KEY, google_cse_id=GOOGLE_CSE_ID),\n",
    "         Text_classification_Tool(llm=llm_text_cls)]\n",
    "# 意图识别agent使用chatglm3 base充当\n",
    "agent = IntentAgent(tools=tools, llm=llm_base)\n",
    "agent_exec = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, max_iterations=1)\n",
    "agent_exec.run(\"Input:今天丢失了我的钱包，里面有很重要的东西，心情很沮丧\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
